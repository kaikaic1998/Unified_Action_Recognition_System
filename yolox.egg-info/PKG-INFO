Metadata-Version: 2.1
Name: yolox
Version: 0.1.0
Summary: UNKNOWN
Home-page: UNKNOWN
Author: basedet team
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
License-File: LICENSE

b'# BoT-SORT\r\n\r\n> [**BoT-SORT: Robust Associations Multi-Pedestrian Tracking**](https://arxiv.org/abs/2206.14651)\r\n> \r\n> Nir Aharon, Roy Orfaig, Ben-Zion Bobrovsky\r\n\r\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bot-sort-robust-associations-multi-pedestrian/multi-object-tracking-on-mot17)](https://paperswithcode.com/sota/multi-object-tracking-on-mot17?p=bot-sort-robust-associations-multi-pedestrian)\r\n\r\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bot-sort-robust-associations-multi-pedestrian/multi-object-tracking-on-mot20-1)](https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=bot-sort-robust-associations-multi-pedestrian)\r\n> \r\n> *[https://arxiv.org/abs/2206.14651](https://arxiv.org/abs/2206.14651)*\r\n\r\n<p align="center"><img src="assets/Results_Bubbles.png"/></p>\r\n\r\n## Highlights \xf0\x9f\x9a\x80\r\n\r\n- YOLOX & YOLOv7 support\r\n- Multi-class support\r\n- Camera motion compensation\r\n- Re-identification\r\n\r\n## Coming Soon\r\n- [ ] Trained YOLOv7 models for MOTChallenge.\r\n- [x] YOLOv7 detector.\r\n- [x] Multi-class support.\r\n- [x] Create OpenCV VideoStab GMC python binding or <u>write Python version<u>.\r\n- [ ] Deployment code.\r\n\r\n## Abstract\r\n\r\nThe goal of multi-object tracking (MOT) is detecting and tracking all the objects in a scene, while keeping a unique identifier for each object. In this paper, we present a new robust state-of-the-art tracker, which can combine the advantages of motion and appearance information, along with camera-motion compensation, and a more accurate Kalman filter state vector. Our new trackers BoT-SORT, and BoT-SORT-ReID rank first in the datasets of MOTChallenge [29, 11] on both MOT17 and MOT20 test sets, in terms of all the main MOT metrics: MOTA, IDF1, and HOTA. For MOT17: 80.5 MOTA, 80.2 IDF1, and 65.0 HOTA are achieved.\r\n\r\n\r\n### Visualization results on MOT challenge test set\r\n\r\n\r\nhttps://user-images.githubusercontent.com/57259165/177045531-947d3146-4d07-4549-a095-3d2daa4692be.mp4\r\n\r\nhttps://user-images.githubusercontent.com/57259165/177048139-05dcb382-010e-41a6-b607-bb2b76afc7db.mp4\r\n\r\nhttps://user-images.githubusercontent.com/57259165/180818066-f67d1f78-515e-4ee2-810f-abfed5a0afcb.mp4\r\n\r\n## Tracking performance\r\n### Results on MOT17 challenge test set\r\n| Tracker       |  MOTA |  IDF1  |  HOTA  |\r\n|:--------------|:-------:|:------:|:------:|\r\n| BoT-SORT      |  80.6   |  79.5  |  64.6  |\r\n| BoT-SORT-ReID |  80.5   |  80.2  |  65.0  |\r\n\r\n### Results on MOT20 challenge test set\r\n| Tracker       | MOTA   | IDF1 | HOTA |\r\n|:--------------|:-------:|:------:|:------:|\r\n|BoT-SORT       | 77.7   | 76.3 | 62.6 | \r\n|BoT-SORT-ReID  | 77.8   | 77.5 | 63.3 | \r\n\r\n\r\n## Installation\r\n\r\nThe code was tested on Ubuntu 20.04\r\n\r\nBoT-SORT code is based on ByteTrack and FastReID. <br>\r\nVisit their installation guides for more setup options.\r\n \r\n### Setup with Anaconda\r\n**Step 1.** Create Conda environment and install pytorch.\r\n```shell\r\nconda create -n botsort_env python=3.7\r\nconda activate botsort_env\r\n```\r\n**Step 2.** Install torch and matched torchvision from [pytorch.org](https://pytorch.org/get-started/locally/).<br>\r\nThe code was tested using torch 1.11.0+cu113 and torchvision==0.12.0 \r\n\r\n**Step 3.** Install BoT-SORT.\r\n```shell\r\ngit clone https://github.com/NirAharon/BoT-SORT.git\r\ncd BoT-SORT\r\npip3 install -r requirements.txt\r\npython3 setup.py develop\r\n```\r\n**Step 4.** Install [pycocotools](https://github.com/cocodataset/cocoapi).\r\n```shell\r\npip3 install cython; pip3 install \'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\'\r\n```\r\n\r\nStep 5. Others\r\n```shell\r\n# Cython-bbox\r\npip3 install cython_bbox\r\n\r\n# faiss cpu / gpu\r\npip3 install faiss-cpu\r\npip3 install faiss-gpu\r\n```\r\n\r\n## Data Preparation\r\n\r\nDownload [MOT17](https://motchallenge.net/data/MOT17/) and [MOT20](https://motchallenge.net/data/MOT20/) from the [official website](https://motchallenge.net/). And put them in the following structure:\r\n\r\n```\r\n<dataets_dir>\r\n      \xe2\x94\x82\r\n      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 MOT17\r\n      \xe2\x94\x82      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\r\n      \xe2\x94\x82      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 test    \r\n      \xe2\x94\x82\r\n      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 MOT20\r\n             \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\r\n             \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 test\r\n```\r\n\r\nFor training the ReID, detection patches must be generated as follows:   \r\n\r\n```shell\r\ncd <BoT-SORT_dir>\r\n\r\n# For MOT17 \r\npython3 fast_reid/datasets/generate_mot_patches.py --data_path <dataets_dir> --mot 17\r\n\r\n# For MOT20\r\n python3 fast_reid/datasets/generate_mot_patches.py --data_path <dataets_dir> --mot 20\r\n```\r\nLink dataset to FastReID ```export FASTREID_DATASETS=<BoT-SORT_dir>/fast_reid/datasets```. If left unset, the default is `fast_reid/datasets` \r\n \r\n## Model Zoo\r\nDownload and store the trained models in \'pretrained\' folder as follow:\r\n```\r\n<BoT-SORT_dir>/pretrained\r\n```\r\n- We used the publicly available [ByteTrack](https://github.com/ifzhang/ByteTrack) model zoo trained on MOT17, MOT20 and ablation study for YOLOX object detection.\r\n\r\n- Ours trained ReID models can be downloaded from [MOT17-SBS-S50](https://drive.google.com/file/d/1QZFWpoa80rqo7O-HXmlss8J8CnS7IUsN/view?usp=sharing), [MOT20-SBS-S50](https://drive.google.com/file/d/1KqPQyj6MFyftliBHEIER7m_OrGpcrJwi/view?usp=sharing).\r\n\r\n- For multi-class MOT use [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) or [YOLOv7](https://github.com/WongKinYiu/yolov7) trained on COCO (or any custom weights). \r\n\r\n## Training\r\n\r\n[//]: # (### Training the Detector)\r\n\r\n[//]: # ()\r\n[//]: # (Please refer to [ByteTrack]&#40;https://github.com/ifzhang/ByteTrack&#41; for training detector.)\r\n\r\n### Train the ReID Module\r\n\r\nAfter generating MOT ReID dataset as described in the \'Data Preparation\' section.\r\n\r\n```shell\r\ncd <BoT-SORT_dir>\r\n\r\n# For training MOT17 \r\npython3 fast_reid/tools/train_net.py --config-file ./fast_reid/configs/MOT17/sbs_S50.yml MODEL.DEVICE "cuda:0"\r\n\r\n# For training MOT20\r\npython3 fast_reid/tools/train_net.py --config-file ./fast_reid/configs/MOT20/sbs_S50.yml MODEL.DEVICE "cuda:0"\r\n```\r\n\r\nRefer to [FastReID](https://github.com/JDAI-CV/fast-reid)  repository for addition explanations and options.\r\n\r\n## Tracking\r\n\r\nBy submitting the txt files produced in this part to [MOTChallenge](https://motchallenge.net/) website and you can get the same results as in the paper.<br>\r\nTuning the tracking parameters carefully could lead to higher performance. In the paper we apply ByteTrack\'s calibration.\r\n\r\n* **Test on MOT17**\r\n\r\n```shell\r\ncd <BoT-SORT_dir>\r\npython3 tools/track.py <dataets_dir/MOT17> --default-parameters --with-reid --benchmark "MOT17" --eval "test" --fp16 --fuse\r\npython3 tools/interpolation.py --txt_path <path_to_track_result>\r\n```\r\n\r\n* **Test on MOT20**\r\n\r\n```shell\r\ncd <BoT-SORT_dir>\r\npython3 tools/track.py <dataets_dir/MOT20> --default-parameters --with-reid --benchmark "MOT20" --eval "test" --fp16 --fuse\r\npython3 tools/interpolation.py --txt_path <path_to_track_result>\r\n```\r\n\r\n* **Evaluation on MOT17 validation set (the second half of the train set)**\r\n\r\n```shell\r\ncd <BoT-SORT_dir>\r\n\r\n# BoT-SORT\r\npython3 tools/track.py <dataets_dir/MOT17> --default-parameters --benchmark "MOT17" --eval "val" --fp16 --fuse\r\n\r\n# BoT-SORT-ReID\r\npython3 tools/track.py <dataets_dir/MOT17> --default-parameters --with-reid --benchmark "MOT17" --eval "val" --fp16 --fuse\r\n```\r\n\r\n* **Other experiments**\r\n\r\nOther parameters can be used __without__ passing --default-parameters flag. <br>\r\nFor evaluating the train and validation sets we recommend using the official MOTChallenge evaluation code from [TrackEval](https://github.com/JonathonLuiten/TrackEval). \r\n\r\n```shell\r\n# For all the available tracking parameters, see:\r\npython3 tools/track.py -h \r\n```\r\n\r\n* **Experiments with YOLOv7**\r\n\r\nOther parameters can be used __without__ passing --default-parameters flag. <br>\r\nFor evaluating the train and validation sets we recommend using the official MOTChallenge evaluation code from [TrackEval](https://github.com/JonathonLuiten/TrackEval). \r\n\r\n```shell\r\n# For all the available tracking parameters, see:\r\npython3 tools/track_yolov7.py -h \r\n```\r\n\r\n## Demo\r\n\r\nDemo with BoT-SORT(-ReID) based YOLOX and multi-class.\r\n\r\n```shell\r\ncd <BoT-SORT_dir>\r\n\r\n# Original example\r\npython3 tools/demo.py video --path <path_to_video> -f yolox/exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar --with-reid --fuse-score --fp16 --fuse --save_result\r\n\r\n# Multi-class example\r\npython3 tools/mc_demo.py video --path <path_to_video> -f yolox/exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar --with-reid --fuse-score --fp16 --fuse --save_result\r\n```\r\n\r\nDemo with BoT-SORT(-ReID) based YOLOv7 and multi-class.\r\n```shell\r\ncd <BoT-SORT_dir>\r\npython3 tools/mc_demo_yolov7.py --weights pretrained/yolov7-d6.pt --source <path_to_video/images> --fuse-score --agnostic-nms (--with-reid)\r\n```\r\n\r\n## Note\r\n\r\nOur camera motion compensation module is based on the OpenCV contrib C++ version of VideoStab Global Motion Estimation, \r\nwhich currently does not have a Python version. <br>\r\nMotion files can be generated using the C++ project called \'VideoCameraCorrection\' in the GMC folder. <br> \r\nThe generated files can be used from the tracker. <br>\r\n\r\nIn addition, python-based motion estimation techniques are available and can be chosen by passing <br> \r\n\'--cmc-method\' <files | orb | ecc> to demo.py or track.py. \r\n\r\n## Citation\r\n\r\n```\r\n@article{aharon2022bot,\r\n  title={BoT-SORT: Robust Associations Multi-Pedestrian Tracking},\r\n  author={Aharon, Nir and Orfaig, Roy and Bobrovsky, Ben-Zion},\r\n  journal={arXiv preprint arXiv:2206.14651},\r\n  year={2022}\r\n}\r\n```\r\n\r\n\r\n## Acknowledgement\r\n\r\nA large part of the codes, ideas and results are borrowed from \r\n[ByteTrack](https://github.com/ifzhang/ByteTrack), \r\n[StrongSORT](https://github.com/dyhBUPT/StrongSORT),\r\n[FastReID](https://github.com/JDAI-CV/fast-reid),\r\n[YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) and\r\n[YOLOv7](https://github.com/wongkinyiu/yolov7). \r\nThanks for their excellent work!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'

